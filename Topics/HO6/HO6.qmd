---
title: "Model Diagnostics"
author: "Wan Kee"
date: "27 January 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  warning: false
  error: true
editor: source
---

![](images/modeldiag.png)

# 1. Learning Objectives

1.  Visualise **model diagnostic** using `performance` package

2.  Visualise **model parameters** using `parameters` package

# 2. Load Packages

The following packages

-   `performance` compute indices of model quality and goodness of fit. It measures r-squared (R2), root mean squared error (RMSE) or intraclass correlation coefficient (ICC) , but also functions to check (mixed) models for overdispersion, zero-inflation, convergence or singularity.

-   `parameters` processes the parameters of various statistical models. It computes p-values, CIs, Bayesian indices and implements features, like bootstrapping of parameters and models, feature reduction (feature extraction and variable selection), or tools for data reduction like functions to perform cluster, factor or principal component analysis.

-   `see` complements numeric summaries to produce visualizations for model parameters, predictions, and performance diagnostics. 

-   `ggstatsplot` creates graphics with statistical tests in the information-rich plots.

```{r}
pacman::p_load(tidyverse, readxl, performance, parameters, see, ggstatsplot)
```

# 3. Import Data

`Toyota Corolla` is a car dataset with a set of explanatory variables.

```{r}
car_resale <- read_xls("data/ToyotaCorolla.xls", "data")
glimpse(car_resale)
```

The output shows 1,436 observations and 38 attributes.

# 4. Model Diagnostic for Multiple Regression Model

`lm()` of `Base Stats` of R calibrates a multiple linear regression model.

```{r}
model <- lm(Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, 
            data = car_resale)
model
```

::: panel-tabset
## Multicolinearity

The first model diagnostic is to check multicolinearity using `check_collinearity()` of `performance` package.

```{r}
check_collinearity(model)
```
**Variance inflation factor (VIF)** quantifies how much the variance of an estimated regression coefficient increases because of collinearity. 

The collinearity plot is divided into **coloured zones**, typically green for low VIF values indicating low collinearity, and red for high VIF values indicating high collinearity. The threshold values for these zones are often set at **5 for low collinearity** and **10 for high collinearity**.

```{r}
check_c <- check_collinearity(model)
plot(check_c)
```

::: {.callout-note icon=false}

## Observation

**Low Collinearity**: Variables such as `Guarantee_Period`, `KM`, and `Weight` are represented by green points and fall in the green zone, suggesting they have low multicollinearity.

**High Collinearity**: The variable `Age_08_04` and `Mfg_Year` is represented by a red point and falls in the red zone, indicating a high VIF and potential issues with multicollinearity.
:::

## Normality Assumption

The second model diagnostic is to check normality assumption using `check_normality()` of `performance` package.

```{r}
model1 <- lm(Price ~ Age_08_04 + KM + Weight + Guarantee_Period, 
             data = car_resale)
model1
```
The **Quantile-Quantile (Q-Q) plot** assess whether the residuals from a regression model are normally distributed. 

The x-axis represents **the quantiles of a standard normal distribution**, which is what the residuals would follow if they were perfectly normally distributed.

The y-axis represents **the sample quantiles of the residuals** from the model. These are the actual observed deviations from the model's predictions.

Typically, a Q-Q plot will have a **reference line** (green) that represents the expected line of points if the residuals were normally distributed. The **dots** are the ordered residual values from the model. If the residuals were perfectly normal, the dots would lie exactly on the reference line.

```{r}
check_n <- check_normality(model1)
plot(check_n)
```

::: {.callout-note icon=false}

## Observation

The middle range of residuals seems to be normally distributed, the presence of outliers and the heavy tails suggest that the model might not be the best fit for all data points, particularly for extreme values.
:::

## Homogeneity of Variances

The third model disgnostic is to check model for **homogeneity of variances** using `check_heteroscedasticity()` of `performance` package.

The homogeneity of variances plot is **a residual versus fitted plot** used to check the assumption of homoscedasticity (equal variances) in the residuals of a regression model.

The x-axis displays the **fitted values**, which are the predicted values generated by the model. In linear regression, you would expect to see a random scatter of points around the horizontal line if the assumption of homoscedasticity holds.

The y-axis shows the **standardized residuals**, which are the model residuals divided by their estimated standard deviation. Ideally, these should be randomly distributed without any pattern.

The **Locally Weighted Scatterplot Smoothing (LOWESS) line** (green) is a non-parametric smoother that helps to visualize the overall trend in the data. In a perfectly homoscedastic scenario, this line would be **flat** and **horizontal**.


```{r}
check_h <- check_heteroscedasticity(model1)
plot(check_h)
```

::: {.callout-note icon=false}

## Observation

The LOWESS line curves upwards as the fitted values increase, suggesting that the variance of the residuals is increasing with the fitted values. This pattern indicates **heteroscedasticity**, meaning that the variance of the residuals is **not constant** across the range of fitted values.

:::

## Model Checking

The forth model disgnostic is to perform a complete check by using `check_model()` of `performance` package to conduct a visual check of various **model assumptions**:

**Posterior Predictive Check** compares the distribution of the observed data with the distribution of the data predicted by the model. The observed data line should closely follow the model-predicted data line if the model is a good fit.

**Linearity** checks if the relationship between predictors and the response is linear.
The reference line should be flat and horizontal if the relationship is linear.

**Homogeneity of Variance (homoscedasticity)** examines whether the residuals have constant variance across all levels of the fitted values. The reference line should be flat and horizontal in the presence of homoscedasticity.

**Influential Observations** plot identifies points that have an undue influence on the model. Ideally, points should be within the contour lines; points outside may unduly influence the model's parameters.

**Collinearity** plot assesses if there is multicollinearity among predictors using the Variance Inflation Factor (VIF). High VIF (generally, VIF > 10) indicates high collinearity which can inflate the variance of the coefficient estimates and make them unstable.

**Normality of Residuals** assesses if the residuals are normally distributed. Points should follow the line in the Q-Q plot if the residuals are normally distributed.


```{r}
check_model(model1)
```

::: {.callout-note icon=false}

## Observation

**Posterior Predictive Check** appears that the predicted values follow the trend of the observed data, indicating that the model is capturing the underlying pattern.

**Linearity** seems to be met, as the residuals are randomly scattered around the horizontal dashed line with no clear pattern.

**Homogeneity of Variance (homoscedasticity)** indicates increasing variability in the residuals as the fitted values increase, which may suggest heteroscedasticity.

**Influential Observations** shows a few points outside the contour lines, suggesting the presence of influential observations that could be affecting the model's estimates.

**Collinearity** is absent as all predictors appear to have low VIF values, indicating that multicollinearity is not a concern.

**Normality of Residuals** shows some deviation from normality, particularly in the tails, indicating potential issues with normality.

:::

:::

# 5. Model Parameters

# 5.1 Estimated Coefficients and Confidence Intervals

`plot()` of `see` package and `parameters()` of `parameters` package is used to visualise the output of a regression model, displaying the **estimated coefficients** and their **confidence intervals** for each predictor variable in the model. 

```{r}
plot(parameters(model1), 
     show_labels = TRUE,
     size_text = 3)
```

::: {.callout-note icon=false}

## Observation

`Age 08 04`: The coefficient for this variable is approximately -119.49, with a confidence interval ranging from -124.91 to -114.08. This suggests that as the age (presumably of a car, given the context) increases by one unit, the price decreases by about 119.49 units on average. The **negative relationship** is statistically significant.

`KM`: The coefficient is very close to zero, around -0.02, with a confidence interval from -0.03 to -0.02. This indicates a **very slight negative relationship** between the number of kilometers driven (KM) and the price. Despite the **small magnitude**, this relationship is statistically significant.

`Weight`: The coefficient for weight is **positive**, at approximately 19.72, with a confidence interval from 18.08 to 21.36. This positive coefficient means that as the weight increases by one unit, the price is expected to increase by about 19.72 units. The positive relationship is statistically significant.

`Guarantee Period`: The coefficient is also **positive**, around 26.82, with a confidence interval from 2.08 to 51.56. This implies that as the guarantee period extends by one unit, the price increases by approximately 26.82 units. Given that the confidence interval is quite wide but does not include zero, the relationship is significant, but with more uncertainty compared to other variables.

:::

# 5.2 Estimated Coefficients, t-values and p-values

`ggcoefstats()` of `ggstatsplot` package to visualise the parameters of a regression model through a plot of a regression analysis output that includes **estimated coefficients** (effect sizes), **t-values**, and **p-values** for each predictor variable, along with the model intercept. 

```{r}
ggcoefstats(model1, output = "plot")
```
::: {.callout-note icon=false}

## Observation

`Intercept`: The model's intercept is significantly different from zero, indicating that when all predictors are at zero, the expected outcome is **negatively offset** by 2185.52 units.

`Age 08 04`: This predictor has a large negative effect on the outcome, with every unit increase in age resulting in a decrease of 119.49 units in the outcome variable. The effect is highly significant, as indicated by the extremely small p-value.

`KM`: Kilometers driven have a small but highly significant negative effect on the outcome, with every additional kilometer driven decreasing the outcome by 0.02 units.

`Weight`: The vehicle's weight has a positive effect on the outcome, with every unit increase in weight leading to an increase of 19.72 units in the outcome variable. This effect is also highly significant.

`Guarantee_Period`: The guarantee period has a positive effect on the outcome. For every unit increase in the guarantee period, the outcome increases by 26.82 units. This effect is significant, albeit less so than the other predictors, as indicated by a p-value of 0.03.
:::